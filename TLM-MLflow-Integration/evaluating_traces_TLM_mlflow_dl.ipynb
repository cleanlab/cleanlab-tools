{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatically find the bad LLM responses in your LLM Evals with Cleanlab\n",
    "\n",
    "This guide will walk you through the process of evaluating LLM responses captured in MLflow with Cleanlab's Trustworthy Language Models (TLM).\n",
    "\n",
    "TLM boosts the reliability of any LLM application by indicating when the modelâ€™s response is untrustworthy.\n",
    "\n",
    "This guide requires a Cleanlab TLM API key. If you don't have one, you can sign up for a free trial [here](https://tlm.cleanlab.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies & Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -q mlflow openai cleanlab-tlm --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Keys\n",
    "\n",
    "This guide requires four API keys:\n",
    "- [OpenAI API Key](https://platform.openai.com/api-keys)\n",
    "- [Cleanlab TLM API Key](https://tlm.cleanlab.ai/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "if not (cleanlab_tlm_api_key := os.getenv(\"CLEANLAB_TLM_API_KEY\")):\n",
    "    cleanlab_tlm_api_key = getpass(\"ðŸ”‘ Enter your Cleanlab TLM API key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "os.environ[\"CLEANLAB_TLM_API_KEY\"] = cleanlab_tlm_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up MLflow Tracking Server and Logging\n",
    "\n",
    "You can start a tutorial and log models, experiments without a tracking server set up. With this mode, your experiment data and artifacts are saved directly under your current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will start a server on port 8080, in the background\n",
    "# Navigate to http://localhost:8080 to see the MLflow UI\n",
    "%%bash --bg\n",
    "mlflow server --host 127.0.0.1 --port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MLflow tracking server\n",
    "mlflow.set_tracking_uri(\"http://localhost:8080\")\n",
    "\n",
    "# Enable logging for OpenAI SDK\n",
    "mlflow.openai.autolog()\n",
    "\n",
    "# Set experiment name\n",
    "mlflow.set_experiment(\"Eval OpenAI Traces w/ TLM\")\n",
    "\n",
    "# Get experiment ID\n",
    "experiment_id = mlflow.get_experiment_by_name(\"Eval OpenAI Traces w/ TLM\").experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare trace dataset and load into MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of demonstration purposes, we'll briefly generate some traces and track them in MLflow. Typically, you would have already captured traces in MLflow and would skip to \"Download trace dataset from MLflow\"\n",
    "\n",
    "NOTE: TLM requires the entire input to the LLM to be provided. This includes any system prompts, context, or other information that was originally provided to the LLM to generate the response. Notice below that we include the system prompt in the trace metadata since by default the trace does not include the system prompt within the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use some tricky trivia questions to generate some traces\n",
    "trivia_questions = [    \n",
    "    \"What is the 3rd month of the year in alphabetical order?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"How many seconds are in 100 years?\",\n",
    "    \"Alice, Bob, and Charlie went to a cafÃ©. Alice paid twice as much as Bob, and Bob paid three times as much as Charlie. If the total bill was $72, how much did each person pay?\",\n",
    "    \"When was the Declaration of Independence signed?\"\n",
    "]\n",
    "\n",
    "def generate_answers(trivia_question):\n",
    "    system_prompt = \"You are a trivia master.\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": trivia_question},\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Generate answers\n",
    "answers = []\n",
    "for i in range(len(trivia_questions)):\n",
    "    answer = generate_answers(trivia_questions[i])\n",
    "    answers.append(answer)  \n",
    "    print(f\"Question {i+1}: {trivia_questions[i]}\")\n",
    "    print(f\"Answer {i+1}:\\n{answer}\\n\")\n",
    "\n",
    "print(f\"Generated {len(answers)} answers and tracked them in MLflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download trace dataset from MLflow\n",
    "\n",
    "Fetching traces from MLflow is straightforward. Just set up the MLflow client and use one of its functions to fetch the data. We'll fetch the traces and evaluate them. After that, we'll add our scores back into MLflow.\n",
    "\n",
    "The `search_traces()` function has arguments to filter the traces by tags, timestamps, and beyond. You can find more about other methods to [query traces](https://mlflow.org/docs/latest/python_api/mlflow.client.html#mlflow.client.MlflowClient.search_traces) in the docs.\n",
    "\n",
    "In this example, we'll fetch all traces from the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.client.MlflowClient()\n",
    "traces = client.search_traces(experiment_ids=[experiment_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate evaluations with TLM\n",
    "\n",
    "Instead of running TLM individually on each trace, we'll provide all of the prompt, response pairs in a list to TLM in a single call. This is more efficient and allows us to get scores and explanations for all of the traces at once. Then, using the `request.id`, we can attach the scores and explanations back to the correct trace in MLflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_tlm import TLM\n",
    "\n",
    "tlm = TLM(options={\"log\": [\"explanation\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper method will extract the prompt and response from each trace and return three lists: request ID's, prompts, and responses.\n",
    "def get_prompt_response_pairs(traces):\n",
    "    prompts = []\n",
    "    responses = []\n",
    "    for trace in traces:\n",
    "        # Parse request and response JSON\n",
    "        request_data = json.loads(trace.data.request)\n",
    "        response_data = json.loads(trace.data.response)\n",
    "        \n",
    "        # Extract system prompt and user message from request\n",
    "        system_prompt = request_data[\"messages\"][0][\"content\"]\n",
    "        user_message = request_data[\"messages\"][1][\"content\"]\n",
    "        \n",
    "        # Extract assistant's response from response\n",
    "        assistant_response = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        prompts.append(system_prompt + \"\\n\" + user_message)\n",
    "        responses.append(assistant_response)\n",
    "    return prompts, responses\n",
    "\n",
    "request_ids = [trace.info.request_id for trace in traces]\n",
    "prompts, responses = get_prompt_response_pairs(traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use TLM to generate a `trustworthiness score` and `explanation` for each trace.\n",
    "\n",
    "**IMPORTANT:** It is essential to always include any system prompts, context, or other information that was originally provided to the LLM to generate the response. You should construct the prompt input to `get_trustworthiness_score()` in a way that is as similar as possible to the original prompt. This is why we included the system prompt in the trace metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each of the prompt, response pairs using TLM\n",
    "evaluations = tlm.get_trustworthiness_score(prompts, responses)\n",
    "\n",
    "# Extract the trustworthiness scores and explanations from the evaluations\n",
    "trust_scores = [entry[\"trustworthiness_score\"] for entry in evaluations]\n",
    "explanations = [entry[\"log\"][\"explanation\"] for entry in evaluations]\n",
    "\n",
    "# Create a DataFrame with the evaluation results\n",
    "trace_evaluations = pd.DataFrame({\n",
    "    'request_id': request_ids,\n",
    "    'prompt': prompts,\n",
    "    'response': responses, \n",
    "    'trust_score': trust_scores,\n",
    "    'explanation': explanations\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now we have a DataFrame mapping trace IDs to their scores and explanations. We've also included the prompt and response for each trace for demonstration purposes to find the **least trustworthy trace!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = trace_evaluations.sort_values(by=\"trust_score\", ascending=True)\n",
    "sorted_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the least trustworthy trace.\n",
    "print(\"Prompt: \", sorted_df.iloc[0][\"prompt\"], \"\\n\")\n",
    "print(\"OpenAI Response: \", sorted_df.iloc[0][\"response\"], \"\\n\")\n",
    "print(\"TLM Trust Score: \", sorted_df.iloc[0][\"trust_score\"], \"\\n\")\n",
    "print(\"TLM Explanation: \", sorted_df.iloc[0][\"explanation\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Awesome! TLM was able to identify multiple traces that contained incorrect answers from OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload the `trust_score` and `explanation` columns to MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload evaluations to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in trace_evaluations.iterrows():\n",
    "    request_id = row[\"request_id\"]\n",
    "    trust_score = row[\"trust_score\"]\n",
    "    explanation = row[\"explanation\"]\n",
    "    \n",
    "    # Add the trustworthiness score and explanation to the trace as a tag\n",
    "    client.set_trace_tag(request_id=request_id, key=\"trust_score\", value=trust_score)\n",
    "    client.set_trace_tag(request_id=request_id, key=\"explanation\", value=explanation)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see the TLM trustworthiness score and explanation in the MLflow UI!\n",
    "\n",
    "\n",
    "From here you can continue collecting and evaluating traces!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator\n",
    "\n",
    "Here's how you might use TLM with MLflow Evaluation. This will log a table of trustworthiness scores and explanations and also provide an interface in the UI for comparing scores across runs. For example, you could use this to compare the trustworthiness scores of different models across the same set of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.metrics import MetricValue, make_metric\n",
    "from cleanlab_tlm import TLM\n",
    "\n",
    "def _tlm_eval_fn(predictions, inputs, targets=None):\n",
    "    \"\"\"\n",
    "    Evaluate trustworthiness using Cleanlab TLM.\n",
    "    \n",
    "    Args:\n",
    "        predictions: The model outputs/answers\n",
    "        targets: Not used for this metric\n",
    "        **kwargs: Should contain 'inputs' with the prompts\n",
    "    \"\"\"\n",
    "    # Initialize TLM\n",
    "    tlm = TLM(options={\"log\": [\"explanation\"]})\n",
    "    inputs = inputs.to_list()\n",
    "    predictions = predictions.to_list()\n",
    "    \n",
    "    # Get trustworthiness scores\n",
    "    evaluations = tlm.get_trustworthiness_score(inputs, predictions)\n",
    "    \n",
    "    # Extract scores and explanations\n",
    "    scores = [float(eval_result[\"trustworthiness_score\"]) for eval_result in evaluations]\n",
    "    justifications = [eval_result[\"log\"][\"explanation\"] for eval_result in evaluations]\n",
    "    \n",
    "    # Return metric value\n",
    "    return MetricValue(\n",
    "        scores=scores,\n",
    "        justifications=justifications,\n",
    "        aggregate_results={\n",
    "            \"mean\": sum(scores) / len(scores),\n",
    "            \"min\": min(scores),\n",
    "            \"max\": max(scores)\n",
    "        }\n",
    "    )\n",
    "\n",
    "def tlm_trustworthiness():\n",
    "    \"\"\"Creates a metric for evaluating trustworthiness using Cleanlab TLM\"\"\"\n",
    "    return make_metric(\n",
    "        eval_fn=_tlm_eval_fn,\n",
    "        greater_is_better=True,\n",
    "        name=\"tlm_trustworthiness\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm_metric = tlm_trustworthiness()\n",
    "\n",
    "eval_df = pd.DataFrame({\n",
    "    'inputs': prompts,\n",
    "    'outputs': answers\n",
    "})\n",
    "\n",
    "\n",
    "results = mlflow.evaluate(\n",
    "    data=eval_df,\n",
    "    predictions=\"outputs\",\n",
    "    model=None,\n",
    "    extra_metrics=[tlm_metric],\n",
    "    evaluator_config={\n",
    "        \"col_mapping\": {\n",
    "            \"inputs\": \"inputs\",\n",
    "            \"predictions\": \"outputs\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing TLM\n",
    "\n",
    "You could also trace the TLM trustworthiness metric itself. This will log the trustworthiness scores and explanations for each trace.\n",
    "\n",
    "Ultimately you would likely want to set this up with a parent span and nested spans, or just entirely separate spans, when the user passes in a list of prompts and responses, perhaps like [this](https://mlflow.org/docs/latest/tracing/api/manual-instrumentation#context-manager). It would also be fairly straightforward to set up a custom MLflow model (or even just a simple function) that invokes the OpenAI model, passes the results to TLM, and traces both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracing TLM\n",
    "\n",
    "@mlflow.trace\n",
    "def tlm_trustworthiness_wrapper(inputs, predictions):\n",
    "    tlm = TLM(options={\"log\": [\"explanation\"]})\n",
    "    evaluations = tlm.get_trustworthiness_score(inputs, predictions)\n",
    "    return evaluations\n",
    "\n",
    "tlm_trustworthiness_wrapper(prompts[0], answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
