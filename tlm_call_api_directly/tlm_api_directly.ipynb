{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to call the TLM REST API directly\n",
    "\n",
    "Although the Trustworthy Language Model officially offers a Python client library and can be used via OpenAI's Python client library, you can still use TLM with another programming language (eg. Typescript) by directly calling TLM's backend REST API.\n",
    "\n",
    "Here we demonstrate how to call the REST API using Python, just for reference. Our code here is simply making http requests, you can use any other programming language with http lib/tools by providing the necessary payload and headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TLM API key, model, and quality preset\n",
    "# More details on models supported and quality presets can be found here: https://help.cleanlab.ai/reference/python/trustworthy_language_model/#class-tlmoptions\n",
    "API_KEY = '<API_KEY>'\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "QUALITY_PRESET = \"medium\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prompt API request to TLM to get back a response and trustworthiness score\n",
    "\n",
    "Note: The `confidence_score` parameter returned in the REST API response is the same as `trustworthiness_score` returned by the Python client library.\n",
    "\n",
    "You can check out the API documentation for more details on inputs and outputs: https://help.cleanlab.ai/tlm/api/python/tlm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "### JSON payload structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'default',\n",
       " 'quality': 'medium',\n",
       " 'prompt': \"What's the first month of the year?\",\n",
       " 'options': {'model': 'gpt-4o-mini', 'log': ['explanation']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"task\": \"default\",\n",
    "  \"quality\": \"medium\",\n",
    "  \"prompt\": \"What's the first month of the year?\",\n",
    "  \"options\": {\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"log\": [\"explanation\"]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs:\n",
    "- `prompt` (required): prompt (or list of prompts) for the TLM to evaluate, inclusive of the user's query and any system instructions.\n",
    "- `task` (optional): determines details of the algorithm used for scoring LLM response trustworthiness, i.e. `default`, `classification`, or `code_generation`.\n",
    "- `quality` (optional): controls the quality of TLM responses and trustworthiness scores vs. latency/costs.\n",
    "- `options` (optional):\n",
    "  - `model` (optional): underlying base LLM to use (better models yield better results, faster models yield faster/cheaper results).\n",
    "  - `log` (optional): optionally specify additional logs or metadata that TLM should return. For instance, include “explanation” here to get explanations of why a response is scored with low trustworthiness.\n",
    "\n",
    "\n",
    "See more [here](https://help.cleanlab.ai/tlm/api/python/tlm/)\n",
    "\n",
    "### Outputs:\n",
    "- `response`: The response from the model.\n",
    "- `confidence_score`: score between 0-1 corresponding to the trustworthiness of the response. A higher score indicates a higher confidence that the response is correct/good.\n",
    "- `explanation`: explanation of why a response is scored with low trustworthiness, if `log` includes `explanation`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confidence_score': 0.9895625234542792,\n",
       " 'log': {'explanation': 'Did not find a reason to doubt trustworthiness.'},\n",
       " 'response': 'The first month of the year is January.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.cleanlab.ai/api/v0/trustworthy_llm/prompt\"\n",
    "\n",
    "\n",
    "def make_prompt_api_request(prompt):\n",
    "    payload = json.dumps({\n",
    "        \"task\": \"default\",\n",
    "        \"quality\": QUALITY_PRESET,\n",
    "        \"prompt\": prompt,\n",
    "        \"options\": {\n",
    "            \"model\": MODEL,\n",
    "            \"log\": [\"explanation\"]\n",
    "        }\n",
    "    })\n",
    "    headers = {\n",
    "        'authorization': f'Bearer {API_KEY}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response_json = requests.request(\"POST\", url, headers=headers, data=payload).json() \n",
    "    # This field is not useful\n",
    "    del response_json['deberta_success']\n",
    "    return response_json\n",
    "\n",
    "make_prompt_api_request(\"What's the first month of the year?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Trusworthiness Score API request to TLM to get back a trustworthiness score\n",
    "\n",
    "Note: The `confidence_score` parameter returned in the REST API response is the same as `trustworthiness_score` returned by the Python client library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confidence_score': 0.018654437417664177,\n",
       " 'log': {'explanation': \"The text expresses a strong positive sentiment towards the product, indicating love for it. Therefore, classifying it as negative is incorrect. A better response would be to classify it as positive. \\nThis response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \\nPositive.\"}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://api.cleanlab.ai/api/v0/trustworthy_llm/get_confidence_score\"\n",
    "\n",
    "def make_score_api_request(prompt, response):\n",
    "    payload = json.dumps({\n",
    "        \"task\": \"classification\",\n",
    "        \"quality\": QUALITY_PRESET,\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response,\n",
    "        \"options\": {\n",
    "            \"model\": MODEL,\n",
    "            \"log\": [\"explanation\"]\n",
    "        }\n",
    "    })\n",
    "    headers = {\n",
    "        'authorization': f'Bearer {API_KEY}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response_json = requests.request(\"POST\", url, headers=headers, data=payload).json()\n",
    "    # This field is not useful\n",
    "    del response_json['deberta_success']\n",
    "    return response_json\n",
    "\n",
    "make_score_api_request(\"Classify this text as positive or negative: 'I love this product!'\", \"negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Trustworthy RAG API request to evaluate your RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following evals are the default, optimized evals Cleanlab uses to evaluate your RAG system. You can choose to use a subset of these evals or even define your own custom eval(s).\n",
    "\n",
    "More details on defining your own custom evals can be found here: https://help.cleanlab.ai/tlm/use-cases/tlm_rag/#custom-evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_EVALS = [\n",
    "    {\n",
    "        \"name\": \"context_sufficiency\",\n",
    "        \"criteria\": \"Determine if the Document contains 100% of the information needed to answer the Question. If any external knowledge or assumptions are required, it does not meet the criteria. Each Question component must have explicit support in the Document.\",\n",
    "        \"query_identifier\": \"Question\",\n",
    "        \"context_identifier\": \"Document\",\n",
    "        \"response_identifier\": None,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"response_groundedness\",\n",
    "        \"criteria\": \"Review the Response to the Query and assess whether every factual claim in the Response is explicitly supported by the provided Context. A Response meets the criteria if all information is directly backed by evidence in the Context, without relying on assumptions, external knowledge, or unstated inferences. The focus is on whether the Response is fully grounded in the Context, rather than whether it fully addresses the Query. If any claim in the Response lacks direct support or introduces information not present in the Context, the Response is bad and does not meet the criteria.\",\n",
    "        \"query_identifier\": \"Query\",\n",
    "        \"context_identifier\": \"Context\",\n",
    "        \"response_identifier\": \"Response\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"response_helpfulness\",\n",
    "        \"criteria\": \"Assess whether the AI Assistant Response is a helpful answer to the User Query. A Response is considered helpful if it makes a genuine attempt to answer the question, even if the answer is incorrect or incomplete. Factual inaccuracies should not affect the assessment. The only thing that matters is whether the Assistant tries to answer the question. A Response is considered not helpful if it avoids answering the question. For example, by saying or implying things like \\\"I don't know\\\", \\\"Sorry\\\", \\\"No information available\\\", or any other form of refusal or deflection.\",\n",
    "        \"query_identifier\": \"User Query\",\n",
    "        \"context_identifier\": None,\n",
    "        \"response_identifier\": \"AI Assistant Response\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"query_ease\",\n",
    "        \"criteria\": \"Determine whether the above User Request appears simple and straightforward. A bad User Request will appear either: ambiguous in intent, complex, purposefully tricky, abnormal, or disgruntled. A good User Request is phrased clearly and expresses an achievable intent. Basic conversational and non-propositional statements are also considered good. Should an AI Assistant be able to properly answer the User Request, it is considered good. The AI Assistant handling this User Request has additional knowledge about: the user, domain-specific terms and abbreviations, and any necessary factual information. So a User Request missing information could still be good; vagueness due to undefined pronouns/terms or references to unknown context does not make a User Request bad.\",\n",
    "        \"query_identifier\": \"User Request\",\n",
    "        \"context_identifier\": None,\n",
    "        \"response_identifier\": None,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs:\n",
    "- `context` (required): retrieved context for the given query\n",
    "- `query` (required): user's query, this is just the question itself\n",
    "- `response` (required): response from the LLM\n",
    "- `prompt` (required): final prompt used to generate the response, inclusive of the context, user's query and any system instructions, in the same format originally used to generate the response\n",
    "- `quality` (optional): controls the quality of TLM responses and trustworthiness scores vs. latency/costs.\n",
    "- `options` (optional):\n",
    "  - `model` (optional): underlying base LLM to use (better models yield better results, faster models yield faster/cheaper results).\n",
    "  - `log` (optional): optionally specify additional logs or metadata that TLM should return. For instance, include “explanation” here to get explanations of why a response is scored with low trustworthiness.\n",
    "\n",
    "### Outputs:\n",
    "- `trustworthiness`: \n",
    "  - `score`: score between 0-1 corresponding to the trustworthiness of the response. A higher score indicates a higher confidence that the response is correct/good.\n",
    "  - `explanation`: explanation of why a response is scored with low trustworthiness, if `log` includes `explanation`.\n",
    "- Selected `evals`:\n",
    "   - `context_sufficiency`: score between 0-1 corresponding to the sufficiency of the context provided to the LLM. Evaluates whether the retrieved context contains sufficient information to answer the query. A low score indicates that key information is missing from the context (useful to diagnose search/retrieval failures or knowledge gaps).\n",
    "  \n",
    "   - `response_groundedness`: score between 0-1 corresponding to the groundedness of the response to the context. Evaluates whether claims/information stated in the response are explicitly supported by the provided context (useful to diagnose when your LLM is fabricating claims or relying on its internal world knowledge over the information retrieved from your knowledge base).\n",
    "   - `response_helpfulness`: score between 0-1 corresponding to the helpfulness of the response to the user's query. Evaluates whether the response attempts to answer the user's query or instead abstain from answering (useful to detect responses unlikely to satisfy the user like generic fallbacks).\n",
    "   - `query_ease`: score between 0-1 corresponding to the ease of the user's query. Evaluates whether the user query seems easy for an AI system to properly handle (useful to diagnose queries that are: complex, vague, tricky, or disgruntled-sounding).\n",
    "\n",
    "See more details [here](https://help.cleanlab.ai/tlm/use-cases/tlm_rag/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_sufficiency': {'score': 0.9975124377784942},\n",
       " 'query_ease': {'score': 0.9975124377422458},\n",
       " 'response_groundedness': {'score': 0.0024875622025382848},\n",
       " 'response_helpfulness': {'score': 0.00696355355661231},\n",
       " 'trustworthiness': {'log': {'explanation': \"Cannot verify that this response is correct.\\nThis response is untrustworthy due to lack of consistency in possible responses from the model. Here's one inconsistent alternate response that the model considered (which may not be accurate either): \\nThe first month of the year is January.\"},\n",
       "  'score': 0.0}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.cleanlab.ai/api/v1/rag_trustworthy_llm/score\"\n",
    "\n",
    "def make_score_rag_api_request(context, query, response, prompt):\n",
    "    payload = json.dumps({ \n",
    "        \"context\": context,\n",
    "        \"query\": query,\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": {\"response\": response},\n",
    "        \"quality\": QUALITY_PRESET,\n",
    "        \"options\": {\n",
    "            \"model\": MODEL,\n",
    "            \"log\": [\"explanation\"]\n",
    "        },\n",
    "        \"evals\": DEFAULT_EVALS\n",
    "    })\n",
    "    headers = {\n",
    "        'authorization': f'Bearer {API_KEY}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response_json = requests.request(\"POST\", url, headers=headers, data=payload).json() \n",
    "    del response_json['deberta_success']\n",
    "    return response_json\n",
    "\n",
    "context = \"The first month of the year is January.\"\n",
    "query = \"What's the first month of the year?\"\n",
    "response = \"February\"\n",
    "prompt = f\"Given the context provided, answer the question. Context: {context} Question: {query} Response:\"\n",
    "\n",
    "make_score_rag_api_request(context, query, response, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Trustworthy RAG API request to generate a response for and evaluate your RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_sufficiency': {'score': 0.9975124377990071},\n",
       " 'query_ease': {'score': 0.9975124377422458},\n",
       " 'response': 'The first month of the year is January.',\n",
       " 'response_groundedness': {'score': 0.9975124378109894},\n",
       " 'response_helpfulness': {'score': 0.9975124378110278},\n",
       " 'trustworthiness': {'log': {'explanation': 'Did not find a reason to doubt trustworthiness.'},\n",
       "  'score': 0.9998661362818028}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.cleanlab.ai/api/v1/rag_trustworthy_llm/generate\"\n",
    "\n",
    "def make_generate_rag_api_request(context, query, prompt):\n",
    "    payload = json.dumps({ \n",
    "        \"context\": context,\n",
    "        \"query\": query,\n",
    "        \"prompt\": prompt,\n",
    "        \"quality\": QUALITY_PRESET,\n",
    "        \"options\": {\n",
    "            \"model\": MODEL,\n",
    "            \"log\": [\"explanation\"]\n",
    "        },\n",
    "        \"evals\": DEFAULT_EVALS\n",
    "    })\n",
    "    headers = {\n",
    "        'authorization': f'Bearer {API_KEY}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response_json = requests.request(\"POST\", url, headers=headers, data=payload).json() \n",
    "    del response_json['deberta_success']\n",
    "    return response_json\n",
    "\n",
    "context = \"The first month of the year is January.\"\n",
    "query = \"What's the first month of the year?\"\n",
    "prompt = f\"Given the context provided, answer the question. Context: {context} Question: {query} Response:\"\n",
    "\n",
    "make_generate_rag_api_request(context, query, prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
