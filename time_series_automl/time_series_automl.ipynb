{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0588ac-64ad-4958-9ddc-3d2c4dfdba41",
   "metadata": {},
   "source": [
    "## How to Forecast Time Series Data using any Supervised Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf99064-bf5b-4e50-8e10-10626509cc46",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mturk24/blog_posts/blob/main/time_series_automl/time_series_automl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84186538-9eb7-449e-8c5d-25694eee9b38",
   "metadata": {},
   "source": [
    "This notebook delves into enhancing the process of forecasting daily energy consumption levels by transforming a time series dataset into a tabular format using open-source libraries. We explore the application of a popular multiclass classification model and leverage AutoML with cleanlab to significantly boost our out-of-sample accuracy.\n",
    "\n",
    "At a high level we will:\n",
    "\n",
    "- Establish a baseline accuracy by fitting a Prophet forecasting model on our time series data\n",
    "- Convert our time series data into a tabular format by using open-source featurization libraries and then will show that can outperform our Prophet model with a standard multiclass classification (Gradient Boosting) approach by a **67% reduction in prediction error** (increase by 38% raw percentage points in out-of-sample accuracy).\n",
    "- Use an AutoML solution for multiclass classification **resulted in a 42% reduction in prediction error** (increase by 8% in raw percentage points in out-of-sample accuracy) compared to our Gradient Boosting model and **resulted in a 81% reduction in prediction error** (increase by 46% in raw percentage points in out-of-sample accuracy) compared to our Prophet forecasting model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ced8a9-59b9-405a-a578-d533cc89f5f6",
   "metadata": {},
   "source": [
    "## Initialize time series data for Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c561683-199a-48ee-97ec-95415fcd7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "data = pd.read_csv('PJME_hourly.csv', parse_dates=['Datetime'], index_col='Datetime')\n",
    "\n",
    "# Assuming pjme_data is loaded as before\n",
    "daily_data = data.resample('D').mean() \n",
    "\n",
    "# Prepare data for Prophet\n",
    "daily_data.reset_index(inplace=True)\n",
    "daily_data.columns = ['ds', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85f6a2-fb50-4234-bca8-af54491acfa5",
   "metadata": {},
   "source": [
    "## Initialize time series data for featurization into a tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd719a8-62a2-45ed-93ba-a9b7288459bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reset the datetime\n",
    "data[\"Datetime\"] = data.index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Create copy for multiclass data \n",
    "df = data.copy()\n",
    "\n",
    "# Convert the datetime column\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])  # Adjust the 'datetime' column name as necessary\n",
    "df = df.sort_values('Datetime').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Obtain day and hour\n",
    "df['Date'] = pd.to_datetime(df['Datetime']).dt.floor('D')  \n",
    "df['Hour'] = pd.to_datetime(df['Datetime']).dt.hour\n",
    "\n",
    "# Create multi-index feature df to compute time series features on\n",
    "features = df.set_index(['Date', 'Hour'])  \n",
    "features.drop(\"Datetime\", inplace=True, axis=1)\n",
    "\n",
    "# Split the data into training and testing sets, respecting the temporal order\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, features[\"PJME_MW\"], test_size=0.2, shuffle=False)\n",
    "\n",
    "# Get group lengths\n",
    "train_lengths = X_train.groupby(level=0).size()\n",
    "test_lengths = X_test.groupby(level=0).size()\n",
    "\n",
    "# Obtain common length value for train/test data\n",
    "train_common_length = train_lengths.mode().iloc[0]\n",
    "test_common_length = test_lengths.mode().iloc[0]\n",
    "\n",
    "# Filter train/test data to groups with same common length for featurizer\n",
    "X_train = X_train.groupby(level=0).filter(lambda x: len(x) == train_common_length)\n",
    "X_test = X_test.groupby(level=0).filter(lambda x: len(x) == test_common_length)\n",
    "\n",
    "# Create quartiles based on training data to avoid leakage\n",
    "quartiles = [X_train['PJME_MW'].quantile(q) for q in [0.25, 0.50, 0.75]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068710b-9f76-4988-a43f-bb8a550c1d6d",
   "metadata": {},
   "source": [
    "## Train and Evaluate Prophet Forecasting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb619ec-f09d-4030-9f28-7a128e434f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape: (4847, 2)\n",
      "Testing Set Shape: (1212, 2)\n"
     ]
    }
   ],
   "source": [
    "# Cutoff date at 2015-04-09\n",
    "cutoff_index = int(len(daily_data) * 0.8)\n",
    "\n",
    "# Use 80% of data for training set and 20% for test set\n",
    "train_df = daily_data.iloc[:cutoff_index]\n",
    "test_df = daily_data.iloc[cutoff_index:]\n",
    "\n",
    "print(\"Training Set Shape:\", train_df.shape)\n",
    "print(\"Testing Set Shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3fd144e-9e93-4b7b-8808-991e52ea40b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>2015-04-05</td>\n",
       "      <td>24577.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>2015-04-06</td>\n",
       "      <td>26996.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>2015-04-07</td>\n",
       "      <td>27177.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>2015-04-08</td>\n",
       "      <td>29136.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4846</th>\n",
       "      <td>2015-04-09</td>\n",
       "      <td>30535.291667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ds             y\n",
       "4842 2015-04-05  24577.500000\n",
       "4843 2015-04-06  26996.666667\n",
       "4844 2015-04-07  27177.833333\n",
       "4845 2015-04-08  29136.041667\n",
       "4846 2015-04-09  30535.291667"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa64b01-ca61-4b8d-ab51-58cb8eb6e964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4847</th>\n",
       "      <td>2015-04-10</td>\n",
       "      <td>29190.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4848</th>\n",
       "      <td>2015-04-11</td>\n",
       "      <td>24774.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4849</th>\n",
       "      <td>2015-04-12</td>\n",
       "      <td>24407.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4850</th>\n",
       "      <td>2015-04-13</td>\n",
       "      <td>26825.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>2015-04-14</td>\n",
       "      <td>26952.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ds             y\n",
       "4847 2015-04-10  29190.166667\n",
       "4848 2015-04-11  24774.291667\n",
       "4849 2015-04-12  24407.625000\n",
       "4850 2015-04-13  26825.333333\n",
       "4851 2015-04-14  26952.125000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c71781e-7e82-407b-8200-a2ce3333ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mturk/mturk-work/cleanlab-tools/time-series-automl-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "13:04:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:04:35 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4249\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize model and train it on training data\n",
    "model = Prophet()\n",
    "model.fit(train_df)\n",
    "\n",
    "# Create a dataframe for future predictions covering the test period\n",
    "future = model.make_future_dataframe(periods=len(test_df), freq='D')\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Categorize forecasted daily values into quartiles based on the thresholds\n",
    "forecast['quartile'] = pd.cut(forecast['yhat'], bins = [-np.inf] + list(quartiles) + [np.inf], labels=[1, 2, 3, 4])\n",
    "\n",
    "# Extract the forecasted quartiles for the test period\n",
    "forecasted_quartiles = forecast.iloc[-len(test_df):]['quartile'].astype(int)\n",
    "\n",
    "\n",
    "# Categorize actual daily values in the test set into quartiles\n",
    "test_df['quartile'] = pd.cut(test_df['y'], bins=[-np.inf] + list(quartiles) + [np.inf], labels=[1, 2, 3, 4])\n",
    "actual_test_quartiles = test_df['quartile'].astype(int)\n",
    "\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "prophet_accuracy = accuracy_score(actual_test_quartiles, forecasted_quartiles)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Accuracy: {prophet_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a556ec87-9280-4271-8be5-65e164c7464e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    3\n",
       "2    3\n",
       "3    3\n",
       "4    2\n",
       "Name: quartile, dtype: category\n",
       "Categories (4, int64): [1 < 2 < 3 < 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For illustrative purposes we show the quartiles for training data\n",
    "train_df['quartile'] = pd.cut(train_df['y'], bins = [-np.inf] + list(quartiles) + [np.inf], labels=[1, 2, 3, 4])\n",
    "train_df['quartile'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b039e99-2ac9-4a36-85b9-f874a5947144",
   "metadata": {},
   "source": [
    "## Convert time series data to tabular format through featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4755933-2bc5-4f18-bb9f-d07769075603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|████████████████████████████████████████████████| 4817/4817 [00:00<00:00, 7909.95it/s]\n",
      "Feature Extraction: 100%|████████████████████████████████████████████████| 1205/1205 [00:00<00:00, 9343.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import tsfel\n",
    "from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
    "\n",
    "# Define tsfresh feature extractor\n",
    "tsfresh_trafo = TSFreshFeatureExtractor(default_fc_parameters=\"minimal\")\n",
    "\n",
    "# Transform the training data using the feature extractor\n",
    "X_train_transformed = tsfresh_trafo.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same feature extractor\n",
    "X_test_transformed = tsfresh_trafo.transform(X_test)\n",
    "\n",
    "# Retrieves a pre-defined feature configuration file to extract all available features\n",
    "cfg = tsfel.get_features_by_domain()\n",
    "\n",
    "# Function to compute tsfel features per day\n",
    "def compute_features(group):\n",
    "    # TSFEL expects a DataFrame with the data in columns, so we transpose the input group\n",
    "    features = tsfel.time_series_features_extractor(cfg, group, fs=1, verbose=0)\n",
    "    return features\n",
    "\n",
    "\n",
    "# Group by the 'day' level of the index and apply the feature computation\n",
    "train_features_per_day = X_train.groupby(level='Date').apply(compute_features).reset_index(drop=True)\n",
    "test_features_per_day = X_test.groupby(level='Date').apply(compute_features).reset_index(drop=True)\n",
    "\n",
    "# Combine each featurization into a set of combined features for our train/test data\n",
    "train_combined_df = pd.concat([X_train_transformed, train_features_per_day], axis=1)\n",
    "test_combined_df = pd.concat([X_test_transformed, test_features_per_day], axis=1)\n",
    "\n",
    "# Filter out features that are highly correlated with our target variable\n",
    "column_of_interest = \"PJME_MW__mean\"\n",
    "train_corr_matrix = train_combined_df.corr()\n",
    "train_corr_with_interest = train_corr_matrix[column_of_interest]\n",
    "null_corrs = pd.Series(train_corr_with_interest.isnull())\n",
    "false_features = null_corrs[null_corrs].index.tolist()\n",
    "\n",
    "columns_to_exclude = list(set(train_corr_with_interest[abs(train_corr_with_interest) > 0.8].index.tolist() + false_features))\n",
    "columns_to_exclude.remove(column_of_interest)\n",
    "\n",
    "# Filtered DataFrame excluding columns with high correlation to the column of interest\n",
    "X_train_transformed = train_combined_df.drop(columns=columns_to_exclude)\n",
    "X_test_transformed = test_combined_df.drop(columns=columns_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d50faef-1dcd-4900-8fe2-15001ccd4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify each value into a quartile\n",
    "def classify_into_quartile(value):\n",
    "    if value < quartiles[0]:\n",
    "        return 1  \n",
    "    elif value < quartiles[1]:\n",
    "        return 2  \n",
    "    elif value < quartiles[2]:\n",
    "        return 3  \n",
    "    else:\n",
    "        return 4  \n",
    "\n",
    "y_train = X_train_transformed[\"PJME_MW__mean\"].rename(\"daily_energy_level\")\n",
    "X_train_transformed.drop(\"PJME_MW__mean\", inplace=True, axis=1)\n",
    "\n",
    "y_test = X_test_transformed[\"PJME_MW__mean\"].rename(\"daily_energy_level\")\n",
    "X_test_transformed.drop(\"PJME_MW__mean\", inplace=True, axis=1)\n",
    "\n",
    "energy_levels_train = y_train.apply(classify_into_quartile)\n",
    "energy_levels_test = y_test.apply(classify_into_quartile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54236393-8463-4c83-8cc0-c28b26db2a5a",
   "metadata": {},
   "source": [
    "## Train and Evaluate GradientBoostingClassifier Model on multiclass tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a34605d9-f596-44e3-a379-2b20febd9d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    min_samples_leaf=20,\n",
    "    max_features='sqrt',\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbc.fit(X_train_transformed, energy_levels_train)\n",
    "\n",
    "\n",
    "y_pred_gbc = gbc.predict(X_test_transformed)\n",
    "gbc_accuracy = accuracy_score(energy_levels_test, y_pred_gbc)\n",
    "print(f'Accuracy: {gbc_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210a381-3159-4891-8cd3-de658d77c06f",
   "metadata": {},
   "source": [
    "## Using AutoML to streamline things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b9036-01c7-4b28-89c8-2f8fb4adffd4",
   "metadata": {},
   "source": [
    "Here’s all the code needed to train and deploy an AutoML supervised classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19edae19-73bc-466a-aa49-865f2a9a915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab_studio import Studio\n",
    "\n",
    "studio = Studio()\n",
    "studio.create_project(\n",
    "    dataset_id=energy_forecasting_dataset,\n",
    "    project_name=\"ENERGY-LEVEL-FORECASTING\",\n",
    "    modality=\"tabular\",\n",
    "    task_type=\"multi-class\",\n",
    "    model_type=\"regular\",\n",
    "    label_column=\"daily_energy_level\",\n",
    ")\n",
    "\n",
    "model = studio.get_model(energy_forecasting_model)\n",
    "y_pred_automl = model.predict(test_data, return_pred_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7d67b0e-6995-4667-accf-c49ab65c7042",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_automl_cleanlab = pd.read_csv(\"quartile-multiclass-pjme-testing-data_pred_probs.csv\")\n",
    "y_pred_automl_cleanlab = y_pred_automl_cleanlab[\"Suggested Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93b0f0b4-6b3d-4c76-8e09-7a285ba312b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8880\n"
     ]
    }
   ],
   "source": [
    "automl_accuracy = accuracy_score(energy_levels_test, y_pred_automl_cleanlab)\n",
    "print(f'Accuracy: {automl_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd61c58e-7880-478f-9bd7-6b889a48d5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet error rate: 0.5750825082508251\n",
      "GBC error rate: 0.1925311203319502\n",
      "AutoML error rate: 0.11203319502074693\n",
      "GBC compared to Prophet resulted in a 66.5% reduction in prediction error.\n",
      "AutoML compared to Prophet resulted in a 80.5% reduction in prediction error.\n",
      "AutoML compared to GBC resulted in a 41.8% reduction in prediction error.\n"
     ]
    }
   ],
   "source": [
    "# Calculate error rates\n",
    "error_rate_prophet = 1 - prophet_accuracy\n",
    "error_rate_gbc = 1 - gbc_accuracy\n",
    "error_rate_automl = 1 - automl_accuracy\n",
    "\n",
    "print(f\"Prophet error rate: {error_rate_prophet}\")\n",
    "print(f\"GBC error rate: {error_rate_gbc}\")\n",
    "print(f\"AutoML error rate: {error_rate_automl}\")\n",
    "\n",
    "# Calculate reduction in prediction error\n",
    "error_reduction_gbc = error_rate_prophet - error_rate_gbc\n",
    "error_reduction_automl_to_prophet = error_rate_prophet - error_rate_automl\n",
    "error_reduction_automl_to_gbc = error_rate_gbc - error_rate_automl\n",
    "\n",
    "# Convert error reduction to a percentage of improvement from one model to another\n",
    "percentage_improvement_gbc = round((error_reduction_gbc / error_rate_prophet) * 100, 1)\n",
    "percentage_improvement_automl_to_prophet = round((error_reduction_automl_to_prophet / error_rate_prophet) * 100, 1)\n",
    "percentage_improvement_automl_to_gbc = round((error_reduction_automl_to_gbc / error_rate_gbc) * 100, 1)\n",
    "\n",
    "print(f\"GBC compared to Prophet resulted in a {percentage_improvement_gbc}% reduction in prediction error.\")\n",
    "print(f\"AutoML compared to Prophet resulted in a {percentage_improvement_automl_to_prophet}% reduction in prediction error.\")\n",
    "print(f\"AutoML compared to GBC resulted in a {percentage_improvement_automl_to_gbc}% reduction in prediction error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4382016-c3af-479d-bd47-7fd6a7dfc055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time-series-automl-env",
   "language": "python",
   "name": "time-series-automl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
