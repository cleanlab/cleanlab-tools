{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textwrap import dedent\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<YOUR_OPENAI_API_KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graded_zs_df = pd.read_csv(\"results/trustworthy_zs_gpt-4.1-mini_graded.csv\")\n",
    "graded_zst_df = pd.read_csv(\"results/trustworthy_zst_gpt-4.1-mini_graded.csv\")\n",
    "graded_react_df = pd.read_csv(\"results/trustworthy_react_gpt-4.1-mini_graded.csv\")\n",
    "graded_planact_df = pd.read_csv(\"results/trustworthy_planact_gpt-4.1-mini_graded.csv\")\n",
    "graded_planreact_df = pd.read_csv(\"results/trustworthy_planreact_gpt-4.1-mini_graded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>step</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>prompt</th>\n",
       "      <th>raw_action</th>\n",
       "      <th>finish_response</th>\n",
       "      <th>trust_score</th>\n",
       "      <th>f1</th>\n",
       "      <th>action_name</th>\n",
       "      <th>action_params</th>\n",
       "      <th>llm_grade</th>\n",
       "      <th>self_eval_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d71ab288-8d4e-4e8b-9c3c-2637c408441f</td>\n",
       "      <td>0</td>\n",
       "      <td>Who was once considered the best kick boxer in...</td>\n",
       "      <td>Badr Hari</td>\n",
       "      <td>You are an intelligent Wikipedia agent. You sh...</td>\n",
       "      <td>Wikipedia_Search[{\"query\": \"best kickboxer con...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.940568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wikipedia_Search</td>\n",
       "      <td>{'query': 'best kickboxer controversies unspor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d71ab288-8d4e-4e8b-9c3c-2637c408441f</td>\n",
       "      <td>1</td>\n",
       "      <td>Who was once considered the best kick boxer in...</td>\n",
       "      <td>Badr Hari</td>\n",
       "      <td>You are an intelligent Wikipedia agent. You sh...</td>\n",
       "      <td>Wikipedia_Search[{\"query\": \"kickboxer controve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.934348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wikipedia_Search</td>\n",
       "      <td>{'query': 'kickboxer controversies unsportsman...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d71ab288-8d4e-4e8b-9c3c-2637c408441f</td>\n",
       "      <td>2</td>\n",
       "      <td>Who was once considered the best kick boxer in...</td>\n",
       "      <td>Badr Hari</td>\n",
       "      <td>You are an intelligent Wikipedia agent. You sh...</td>\n",
       "      <td>Wikipedia_Search[{\"query\": \"kickboxer best in ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.892140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wikipedia_Search</td>\n",
       "      <td>{'query': 'kickboxer best in the world controv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d71ab288-8d4e-4e8b-9c3c-2637c408441f</td>\n",
       "      <td>3</td>\n",
       "      <td>Who was once considered the best kick boxer in...</td>\n",
       "      <td>Badr Hari</td>\n",
       "      <td>You are an intelligent Wikipedia agent. You sh...</td>\n",
       "      <td>Wikipedia_Search[{\"query\": \"best kickboxer con...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.902625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wikipedia_Search</td>\n",
       "      <td>{'query': 'best kickboxer controversies violen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d71ab288-8d4e-4e8b-9c3c-2637c408441f</td>\n",
       "      <td>4</td>\n",
       "      <td>Who was once considered the best kick boxer in...</td>\n",
       "      <td>Badr Hari</td>\n",
       "      <td>You are an intelligent Wikipedia agent. You sh...</td>\n",
       "      <td>Wikipedia_Search[{\"query\": \"best kickboxer con...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.926903</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wikipedia_Search</td>\n",
       "      <td>{'query': 'best kickboxer controversies violen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                task_id  step  \\\n",
       "0  d71ab288-8d4e-4e8b-9c3c-2637c408441f     0   \n",
       "1  d71ab288-8d4e-4e8b-9c3c-2637c408441f     1   \n",
       "2  d71ab288-8d4e-4e8b-9c3c-2637c408441f     2   \n",
       "3  d71ab288-8d4e-4e8b-9c3c-2637c408441f     3   \n",
       "4  d71ab288-8d4e-4e8b-9c3c-2637c408441f     4   \n",
       "\n",
       "                                            question     answer  \\\n",
       "0  Who was once considered the best kick boxer in...  Badr Hari   \n",
       "1  Who was once considered the best kick boxer in...  Badr Hari   \n",
       "2  Who was once considered the best kick boxer in...  Badr Hari   \n",
       "3  Who was once considered the best kick boxer in...  Badr Hari   \n",
       "4  Who was once considered the best kick boxer in...  Badr Hari   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  You are an intelligent Wikipedia agent. You sh...   \n",
       "1  You are an intelligent Wikipedia agent. You sh...   \n",
       "2  You are an intelligent Wikipedia agent. You sh...   \n",
       "3  You are an intelligent Wikipedia agent. You sh...   \n",
       "4  You are an intelligent Wikipedia agent. You sh...   \n",
       "\n",
       "                                          raw_action finish_response  \\\n",
       "0  Wikipedia_Search[{\"query\": \"best kickboxer con...             NaN   \n",
       "1  Wikipedia_Search[{\"query\": \"kickboxer controve...             NaN   \n",
       "2  Wikipedia_Search[{\"query\": \"kickboxer best in ...             NaN   \n",
       "3  Wikipedia_Search[{\"query\": \"best kickboxer con...             NaN   \n",
       "4  Wikipedia_Search[{\"query\": \"best kickboxer con...             NaN   \n",
       "\n",
       "   trust_score  f1       action_name  \\\n",
       "0     0.940568 NaN  Wikipedia_Search   \n",
       "1     0.934348 NaN  Wikipedia_Search   \n",
       "2     0.892140 NaN  Wikipedia_Search   \n",
       "3     0.902625 NaN  Wikipedia_Search   \n",
       "4     0.926903 NaN  Wikipedia_Search   \n",
       "\n",
       "                                       action_params llm_grade  \\\n",
       "0  {'query': 'best kickboxer controversies unspor...       NaN   \n",
       "1  {'query': 'kickboxer controversies unsportsman...       NaN   \n",
       "2  {'query': 'kickboxer best in the world controv...       NaN   \n",
       "3  {'query': 'best kickboxer controversies violen...       NaN   \n",
       "4  {'query': 'best kickboxer controversies violen...       NaN   \n",
       "\n",
       "   self_eval_score  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graded_zs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_confidence_prompt(full_prompt_and_actions: str, finish_action: str) -> str:\n",
    "    template = f\"\"\"\n",
    "Interaction:\n",
    "{full_prompt_and_actions}{finish_action}\n",
    "\n",
    "Instruction:\n",
    "Evaluate whether the Finish action's \"response\" correctly and completely answers the task described in the Interaction.\n",
    "\n",
    "Output (strictly):\n",
    "Explanation: [one- or two-sentence rationale]\n",
    "Correct: [Yes or No]\"\"\"\n",
    "    return dedent(template)\n",
    "\n",
    "def parse_self_eval_score(text: str) -> float:\n",
    "    t = text.lower()\n",
    "    if \"correct: yes\" in t:\n",
    "        return 1.0\n",
    "    if \"correct: no\" in t:\n",
    "        return 0.0\n",
    "    return np.nan\n",
    "\n",
    "def get_self_eval_score(llm, full_prompt_and_actions: str, finish_action: str) -> float:\n",
    "    \"\"\"Run the LLM and parse out a 1.0/0.0 score.\"\"\"\n",
    "    prompt = construct_confidence_prompt(full_prompt_and_actions, finish_action)\n",
    "    resp = llm(prompt)\n",
    "    return parse_self_eval_score(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "def add_self_eval_scores(\n",
    "    df: pd.DataFrame,\n",
    "    llm,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a new DataFrame with a `self_eval_score` column.\n",
    "\n",
    "    - Uses `prompt` as the full interaction text for every row.\n",
    "    - Runs `get_self_eval_score` on rows where `action_name` == 'Finish'.\n",
    "    - Leaves NaN elsewhere.\n",
    "    \"\"\"\n",
    "    def _compute(row):\n",
    "        if row[\"action_name\"] == \"Finish\":\n",
    "            return get_self_eval_score(\n",
    "                llm,\n",
    "                row[\"prompt\"],\n",
    "                row[\"raw_action\"]\n",
    "            )\n",
    "        return np.nan\n",
    "\n",
    "    df = df.copy()\n",
    "    # this will show a tqdm bar in the notebook\n",
    "    df[\"self_eval_score\"] = df.progress_apply(_compute, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def llm(prompt: str) -> str:\n",
    "    reply = openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[{\"role\":\"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                    {\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    return reply.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an example of a self-evaluation prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interaction:\n",
      "You are an intelligent Wikipedia agent. You should follow your [Role], [Action_Doc] to take actions. You should decompose your task into executable actions. Your generation should follow the example format. Finish the task if you find the answer. And your answer should be simple and straighforward. You can only take one action per generation. DO NOT repeat your actions.\n",
      "[Role]\n",
      "Answer questions by searching Wikipedia content.\n",
      "[End of Role]\n",
      "[Constraint]\n",
      "Generation should be simple and clear.\n",
      "[End of Constraint]\n",
      "[Action_Doc]\n",
      "[{'name': 'Wikipedia_Search', 'description': 'Using this API to search Wiki content.', 'parameters': {'query': 'the search string. be simple.'}}, {'name': 'Finish', 'description': 'Complete the task with a response.', 'parameters': {'response': 'this is the finish action response. Respond towards the task instruction.'}}]\n",
      "[End of Action_Doc]\n",
      "Using the following action format example to generate well formatted actions.\n",
      "[ActionFormatExample]\n",
      "Action: Wikipedia_Search[{\"query\": \"the search string. be simple.\"}]\n",
      "Action: Finish[{\"response\": \"this is the finish action response. Respond towards the task instruction.\"}]\n",
      "[End of ActionFormatExample]\n",
      "[Execution]\n",
      "Task: In which American football game was Malcolm Smith named Most Valuable player?\n",
      "Action: Wikipedia_Search[{\"query\": \"Malcolm Smith Most Valuable Player American football game\"}]\n",
      "Observation: Malcolm Xavier Smith (born July 5, 1989) is an American former professional football player who was a linebacker in the National Football League (NFL). He played college football for the USC Trojans. He was selected by the Seattle Seahawks in the seventh round of the 2011 NFL draft. Smith was named the Most Valuable Player of Super Bowl XLVIII after the Seahawks' victory over the Denver Broncos.\n",
      "\n",
      "Action: Finish[{\"response\": \"Malcolm Smith was named Most Valuable Player in Super Bowl XLVIII.\"}]\n",
      "\n",
      "Instruction:\n",
      "Evaluate whether the Finish action's \"response\" correctly and completely answers the task described in the Interaction.\n",
      "\n",
      "Output (strictly):\n",
      "Explanation: [one- or two-sentence rationale]\n",
      "Correct: [Yes or No]\n"
     ]
    }
   ],
   "source": [
    "finish_row = graded_zs_df[graded_zs_df['action_name'] == 'Finish'].iloc[0]\n",
    "\n",
    "full_prompt       = finish_row['prompt']\n",
    "finish_action_raw = finish_row['raw_action']\n",
    "\n",
    "print(construct_confidence_prompt(full_prompt, finish_action_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af13cfaaa90549f2a1b6d194233e4ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1941 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graded_planreact_df = add_self_eval_scores(graded_planreact_df, llm)\n",
    "graded_planreact_df.to_csv(\"results/trustworthy_planreact_gpt-4.1-mini_graded.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentlite_benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
