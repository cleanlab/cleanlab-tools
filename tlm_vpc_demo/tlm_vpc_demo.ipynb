{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "626216e6",
   "metadata": {},
   "source": [
    "# Using TLM with OpenAI's Chat Completions API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82efd2ae",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to integrate your VPC installation of Cleanlab's Trustworthy Language Model (TLM) into existing GenAI apps. You will learn how to assess the trustworthiness of LLM model responses, directly through the [OpenAI client library](https://github.com/openai/openai-python) or Cleanlab's `cleanlab-tlm` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e356a1",
   "metadata": {},
   "source": [
    "## API access to the TLM backend service\n",
    "\n",
    "This demo assumes that you have access to the deployed TLM backend service at the URL `http://example.customer.com:8080/api`. You are welcome to expose the TLM service however you prefer, depending on the unique needs of your networking environment. Simply replace the base URL in the corresponding cell blocks below.\n",
    "\n",
    "Please note that Google Colab does **_not_** have built-in support to access services on your local machine. This is because Colab [runs in a virtual machine](https://research.google.com/colaboratory/faq.html#executed-code), so `localhost` refers to that VM, rather than your computer. If you would like to access TLM by port-forwarding to your local machine, you may do so by downloading the `.ipynb` file and running Jupyter locally, or by using a tunneling service like [ngrok](https://ngrok.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f00de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"BASE_URL\"] = \"http://example.customer.com:8080/api\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1efe315",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The Python packages required for this tutorial can be installed using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "828db034",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade openai\n",
    "%pip install --upgrade cleanlab-tlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16ace027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "from cleanlab_tlm.utils.vpc.chat_completions import TLMChatCompletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f07fc",
   "metadata": {},
   "source": [
    "## Overview of this tutorial\n",
    "\n",
    "The workflows showcased below demonstrates how to incorporate trust scoring into your existing LLM code with minimal code changes. We'll explore three workflows:\n",
    "\n",
    "- Workflow 1 & 2: Use your own existing LLM infrastructure to generate responses, then use Cleanlab to score them\n",
    "- Workflow 3: Use Cleanlab for both generating and scoring responses (response-generation can be from any LLM model supported in your VPC deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e12122",
   "metadata": {},
   "source": [
    "## Workflow 1: Score Responses from Existing LLM Calls\n",
    "\n",
    "The easiest way to use TLM if you're already using OpenAI's ChatCompletions API is to score any existing LLM call you've made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836451af",
   "metadata": {},
   "source": [
    "You can first obtain generate LLM responses as usual using the OpenAI API (or any of your existing infrastructure, note that many LLM providers like Gemini/DeepSeek also support OpenAI's Chat Completions API):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88348955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-Bjuu48x0SxfeGuz3GY3jI2TpeAEUK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750283196, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_6f2eabb9a5', usage=CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_kwargs = {\n",
    "    \"model\": \"gpt-4.1-mini\",\n",
    "    \"messages\":[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    "}\n",
    "response = openai.chat.completions.create(**openai_kwargs)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc414d2-8e73-4891-8b7a-665a91767dfc",
   "metadata": {},
   "source": [
    "We can then use TLM to score the generated response.\n",
    "\n",
    "Here, we first instantiate a `TLMChatCompletion` object. For more configurations, view the valid arguments [below](#input-arguments-to-tlm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f9b5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = TLMChatCompletion(quality_preset=\"medium\", options={\"model\": \"gpt-4o-mini\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb8c3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of France is Paris.\n",
      "TLM Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "score_result = tlm.score(\n",
    "    response=response,\n",
    "    **openai_kwargs\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(f\"TLM Score: {score_result['trustworthiness_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f54ce",
   "metadata": {},
   "source": [
    "## Workflow 2: Adding a Decorator to your LLM Call\n",
    "\n",
    "Alternatively, you decorate your call to `openai.chat.completions.create()` with a decorator that then appends the trust score as a key in the returned response. This workflow only requires an initial setup which then requires zero changes to the rest of your existing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "710cfbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def add_trust_scoring(tlm_instance):\n",
    "    \"\"\"Decorator factory that creates a trust scoring decorator.\"\"\"\n",
    "    def trust_score_decorator(fn):\n",
    "        @functools.wraps(fn)\n",
    "        def wrapper(**kwargs):\n",
    "            response = fn(**kwargs)\n",
    "            score_result = tlm_instance.score(response=response, **kwargs)\n",
    "            response.tlm_metadata = score_result\n",
    "            return response\n",
    "        return wrapper\n",
    "    return trust_score_decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524021e9-4e45-4648-98eb-72400c66d540",
   "metadata": {},
   "source": [
    "Then, we can decorate the OpenAI client, and then your existing code automatically gets trust scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b4a57e-9fce-4011-9046-81baf172e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = TLMChatCompletion(quality_preset=\"medium\", options={\"model\": \"gpt-4.1-mini\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8aa301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-Bjuu831HubejCal4AsdB2UWeLEJW2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750283200, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_6f2eabb9a5', usage=CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), tlm_metadata={'trustworthiness_score': 0.9999999233293982})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.chat.completions.create = add_trust_scoring(tlm)(openai.chat.completions.create)\n",
    "\n",
    "response = openai.chat.completions.create(**openai_kwargs)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "445c199c-ef55-4860-9337-38f8c573a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of France is Paris.\n",
      "TLM Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(f\"TLM Score: {response.tlm_metadata['trustworthiness_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b6420",
   "metadata": {},
   "source": [
    "## Workflow 3: Use Cleanlab to Generate and Score Responses\n",
    "\n",
    "You can point the OpenAI client directly to Cleanlab's infrastructure. This approach generates responses using Cleanlab's backend while simultaneously providing trustworthiness scores.\n",
    "\n",
    "Here, you can replace the base URL with your actual TLM service endpoint, and then use the `chat.completions.create()` method as you normally would:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55f0983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\".\",  # the VPC installation of TLM does not require API key, but the OpenAI client does, so we pass a fake value here\n",
    "    base_url=\"http://example.customer.com:8080/api\"  # replace with your TLM service URL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c44e262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BjuuC9gCQQk7yfonPQ3upUQsBef3v', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750283204, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_34a54ae93c', usage=CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), tlm_metadata={'trustworthiness_score': 0.9999997198696753})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"quality_preset\": \"low\"\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9d38b",
   "metadata": {},
   "source": [
    "The `extra_body` argument contains additional TLM configurations. For all supported inputs, view the valid arguments [below](#input-arguments-to-tlm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30f94554-8d95-4c44-aa46-402780a4e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of France is Paris.\n",
      "TLM Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Response: {response.choices[0].message.content}\")\n",
    "print(f\"TLM Score: {response.tlm_metadata['trustworthiness_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01aebb",
   "metadata": {},
   "source": [
    "### Adding a decorator to pass in TLM configurations via `extra_body`\n",
    "\n",
    "Here, we demonstrate how to decorate your call to `openai.chat.completions.create()` which will automatically add the `extra_body` argument to all your subsequent calls to the `create()` method, which after the initial setup will require zero changes to your existing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "348bc749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def add_extra_body(tlm_kwargs):\n",
    "    def decorator(fn):\n",
    "        @functools.wraps(fn)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            kwargs[\"extra_body\"] = tlm_kwargs\n",
    "            return fn(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262bbea",
   "metadata": {},
   "source": [
    "Similar to above, we can decorate the OpenAI client. After this monkey-patch, the code below is functionally equivalent to the one above where we specified `extra_body` in each `create()` call -- this make it such that you can use your existing code with minimal changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "939fac06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BjuuGxSKNvPWoQqYL8iSNC6e8UYjt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750283208, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_34a54ae93c', usage=CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), tlm_metadata={'trustworthiness_score': 0.9999997198696664})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlm_kwargs = {\"quality_preset\": \"low\"}\n",
    "client.chat.completions.create = add_extra_body(tlm_kwargs)(client.chat.completions.create)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ],\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcdd048",
   "metadata": {},
   "source": [
    "## Input Arguments to TLM\n",
    "\n",
    "These are optional TLM configurations you can specify either when initializing `TLMChatCompletion` object, or in the `extra_body` argument to the OpenAI API client.\n",
    "\n",
    "- `quality_preset` ({\"base\", \"low\", \"medium\"}, default = \"medium\"): a preset configuration to control the quality of TLM responses and trustworthiness scores vs. latency/costs. The \"medium\" preset produces more reliable trustworthiness scores than \"low\". The \"base\" preset provides the lowest possible latency/cost. Higher presets have increased runtime and cost. Reduce your preset if you see token-limit errors. \n",
    "\n",
    "- `options` is a dictionary of configuration options for TLM. Inputs include:\n",
    "    - `model` (default = “gpt-4.1-mini”): Underlying base LLM to use (better models yield better results, faster models yield faster results). \n",
    "    \n",
    "        Note that if you are using the OpenAI `openai.chat.completions.create()` API, you should provide the model name there instead of in the options dictionary here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274acc8",
   "metadata": {},
   "source": [
    "## Getting Cheaper / Faster Results\n",
    "\n",
    "The default TLM settings are not latency-optimized because they have to remain effective across all possible LLM use-cases. For your specific use-case, you can greatly improve latency without compromising results. Strategy: first run TLM with default settings to see what results look like over a dataset from your use-case; once results look promising, adjust the TLM preset/options/model to reduce latency for your application.\n",
    "\n",
    "- You can stream in a response from any (fast) LLM you are using, and then use `TLMChatCompletion.score` to subsequently stream in the trustworthiness score for the response. If you run TLM with a lower `quality_preset` and cheaper model, then the additional cost/runtime of trustworthiness scoring can be only a fraction of your cost/runtime of producing the response with your own LLM.\n",
    "\n",
    "- Reduce the quality_preset setting (e.g. to \"low\" or \"base:).\n",
    "\n",
    "- Specify `options` to further reduce TLM runtimes by: changing model to a faster base LLM (e.g. `gpt-4.1-nano`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc42444",
   "metadata": {},
   "source": [
    "## Running on Batches and Managing Rate Limits\n",
    "\n",
    "When processing large datasets, here are some tips to handle rate limits and implement proper batching strategies:\n",
    "\n",
    "### Prevent hitting rate limits\n",
    "- Process data in small batches (e.g. 10-50 requests at a time)\n",
    "- Add sleep intervals between batches (e.g. `time.sleep(1)`) to stay under rate limits\n",
    "\n",
    "### Handling errors\n",
    "- Save partial results frequently to avoid losing progress\n",
    "- Consider using a try/except block to catch errors, and implement retry logic when rate limits are hit\n",
    "\n",
    "Here are some sample helper functions that could help with batching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\".\",  # the VPC installation of TLM does not require API key, but the OpenAI client does, so we pass a fake value here\n",
    "    base_url=\"http://example.customer.com:8080/api\"  # replace with your TLM service URL\n",
    ")\n",
    "\n",
    "def invoke_llm_with_retries(openai_kwargs, retries=3, backoff=2):\n",
    "    attempt = 0\n",
    "    while attempt <= retries:\n",
    "        try:\n",
    "            # the code to invoke the LLM goes here, feel free to modify\n",
    "            response = client.chat.completions.create(**openai_kwargs)\n",
    "            return {\n",
    "                \"response\": response.choices[0].message.content,\n",
    "                \"trustworthiness_score\": response.tlm_metadata[\"trustworthiness_score\"],\n",
    "                \"raw_completion\": response\n",
    "            }\n",
    "        except Exception as e:\n",
    "            if attempt == retries:\n",
    "                return {\"error\": str(e), \"input\": openai_kwargs}\n",
    "            sleep_time = backoff ** attempt\n",
    "            time.sleep(sleep_time)\n",
    "            attempt += 1\n",
    "\n",
    "def run_batch(batch_data, batch_size=20, max_threads=8, sleep_time=5):\n",
    "    results = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(batch_data), batch_size)):\n",
    "        data = batch_data[i:i + batch_size]\n",
    "        batch_results = [None] * len(data)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "            future_to_idx = {executor.submit(invoke_llm_with_retries, d): idx for idx, d in enumerate(data)}\n",
    "            for future in as_completed(future_to_idx):\n",
    "                idx = future_to_idx[future]\n",
    "                batch_results[idx] = future.result()\n",
    "                \n",
    "        results.extend(batch_results)\n",
    "\n",
    "        # sleep to prevent hitting rate limits\n",
    "        if i + batch_size < len(batch_data):\n",
    "            time.sleep(sleep_time)\n",
    "            \n",
    "    return results\n",
    "\n",
    "sample_input = {\n",
    "    \"model\": \"gpt-4.1-mini\",\n",
    "    \"messages\":[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    "}\n",
    "sample_batch = [sample_input] * 10\n",
    "run_batch(sample_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e68e267",
   "metadata": {},
   "source": [
    "More information about handling rate limits can be found in [this OpenAI cookbook](https://cookbook.openai.com/examples/how_to_handle_rate_limits)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
