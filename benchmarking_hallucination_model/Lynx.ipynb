{"cells":[{"cell_type":"markdown","metadata":{"id":"LvDkrXSDetJW"},"source":["# Lynx\n","\n"]},{"cell_type":"markdown","metadata":{"id":"92S7AJpY2B97"},"source":["You'd need a GPU with atleast 32GB RAM in order to load the smallest Lynx model i.e. 8B FP32.\n","\n","You'd need atleast 4 GPUs with 40GB RAM each to load the bigger Lynx model i.e. 70B FP32."]},{"cell_type":"markdown","source":["Install dependencies"],"metadata":{"id":"HXSpWGvxDaSz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jrPB4JSy2HMi"},"outputs":[],"source":["%pip install transformers\n","%pip install datasets --quiet"]},{"cell_type":"markdown","source":["Load model"],"metadata":{"id":"0hbVZ6B7DccU"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["204fd7a727244f01a28922afed0344cf"]},"id":"o5GzqOTzCNj-","outputId":"cd0135b3-e791-460c-94bd-c0dde2ff5bd0"},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO 12-16 05:42:19 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n","INFO 12-16 05:42:20 config.py:1020] Defaulting to use mp for distributed inference\n","INFO 12-16 05:42:20 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='PatronusAI/Llama-3-Patronus-Lynx-70B-Instruct', speculative_config=None, tokenizer='PatronusAI/Llama-3-Patronus-Lynx-70B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=PatronusAI/Llama-3-Patronus-Lynx-70B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n","WARNING 12-16 05:42:21 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n","INFO 12-16 05:42:21 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n","INFO 12-16 05:42:21 selector.py:135] Using Flash Attention backend.\n","\u001b[1;36m(VllmWorkerProcess pid=3496)\u001b[0;0m INFO 12-16 05:42:21 selector.py:135] Using Flash Attention backend.\n","\u001b[1;36m(VllmWorkerProcess pid=3497)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3495)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3496)\u001b[0;0m INFO 12-16 05:42:21 selector.py:135] Using Flash Attention backend.\n","INFO 12-16 05:42:21 selector.py:135] Using Flash Attention backend.\n","INFO 12-16 05:42:21 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n","\u001b[1;36m(VllmWorkerProcess pid=3497)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3495)\u001b[0;0m INFO 12-16 05:42:21 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n","INFO 12-16 05:42:21 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n","INFO 12-16 05:42:24 utils.py:961] Found nccl from library libnccl.so.2\n","INFO 12-16 05:42:24 pynccl.py:69] vLLM is using nccl==2.21.5\n","\u001b[1;36m(VllmWorkerProcess pid=3495)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3496)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3497)\u001b[0;0m INFO 12-16 05:42:24 utils.py:961] Found nccl from library libnccl.so.2\n","INFO 12-16 05:42:24 utils.py:961] Found nccl from library libnccl.so.2\n","INFO 12-16 05:42:24 utils.py:961] Found nccl from library libnccl.so.2\n","\u001b[1;36m(VllmWorkerProcess pid=3495)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3496)\u001b[0;0m INFO 12-16 05:42:24 pynccl.py:69] vLLM is using nccl==2.21.5\n","INFO 12-16 05:42:24 pynccl.py:69] vLLM is using nccl==2.21.5\n","\u001b[1;36m(VllmWorkerProcess pid=3497)\u001b[0;0m INFO 12-16 05:42:24 pynccl.py:69] vLLM is using nccl==2.21.5\n","INFO 12-16 05:42:26 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/asardana/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n","\u001b[1;36m(VllmWorkerProcess pid=3497)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3496)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3495)\u001b[0;0m INFO 12-16 05:42:26 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/asardana/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n","INFO 12-16 05:42:26 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/asardana/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n","INFO 12-16 05:42:26 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/asardana/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n","INFO 12-16 05:42:26 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fc5b3450490>, local_subscribe_port=56725, remote_subscribe_port=None)\n","INFO 12-16 05:42:26 model_runner.py:1072] Starting to load model PatronusAI/Llama-3-Patronus-Lynx-70B-Instruct...\n","\u001b[1;36m(VllmWorkerProcess pid=3496)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3495)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3497)\u001b[0;0m INFO 12-16 05:42:26 model_runner.py:1072] Starting to load model PatronusAI/Llama-3-Patronus-Lynx-70B-Instruct...\n","INFO 12-16 05:42:26 model_runner.py:1072] Starting to load model PatronusAI/Llama-3-Patronus-Lynx-70B-Instruct...\n","INFO 12-16 05:42:26 model_runner.py:1072] Starting to load model PatronusAI/Llama-3-Patronus-Lynx-70B-Instruct...\n","\u001b[1;36m(VllmWorkerProcess pid=3495)\u001b[0;0m INFO 12-16 05:42:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n","\u001b[1;36m(VllmWorkerProcess pid=3497)\u001b[0;0m INFO 12-16 05:42:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n","INFO 12-16 05:42:27 weight_utils.py:243] Using model weights format ['*.safetensors']\n","\u001b[1;36m(VllmWorkerProcess pid=3496)\u001b[0;0m INFO 12-16 05:42:27 weight_utils.py:243] Using model weights format ['*.safetensors']\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"204fd7a727244f01a28922afed0344cf","version_major":2,"version_minor":0},"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/30 [00:00<?, ?it/s]\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["INFO 12-16 05:51:31 model_runner.py:1077] Loading model weights took 32.8599 GB\n","\u001b[1;36m(VllmWorkerProcess pid=3495)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3497)\u001b[0;0m INFO 12-16 05:51:31 model_runner.py:1077] Loading model weights took 32.8599 GB\n","INFO 12-16 05:51:31 model_runner.py:1077] Loading model weights took 32.8599 GB\n","\u001b[1;36m(VllmWorkerProcess pid=3496)\u001b[0;0m INFO 12-16 05:51:31 model_runner.py:1077] Loading model weights took 32.8599 GB\n","\u001b[1;36m(VllmWorkerProcess pid=3496)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3497)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3495)\u001b[0;0m INFO 12-16 05:51:35 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=34.19GiB peak_torch_memory=33.58GiB memory_usage_post_profile=35.11GiB non_torch_memory=2.24GiB kv_cache_size=2.78GiB gpu_memory_utilization=0.98\n","INFO 12-16 05:51:35 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=34.33GiB peak_torch_memory=33.58GiB memory_usage_post_profile=35.39GiB non_torch_memory=2.52GiB kv_cache_size=2.50GiB gpu_memory_utilization=0.98\n","INFO 12-16 05:51:35 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=34.33GiB peak_torch_memory=33.58GiB memory_usage_post_profile=35.39GiB non_torch_memory=2.52GiB kv_cache_size=2.50GiB gpu_memory_utilization=0.98\n","INFO 12-16 05:51:35 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=34.19GiB peak_torch_memory=34.16GiB memory_usage_post_profile=35.49GiB non_torch_memory=2.61GiB kv_cache_size=1.82GiB gpu_memory_utilization=0.98\n","INFO 12-16 05:51:36 distributed_gpu_executor.py:57] # GPU blocks: 1488, # CPU blocks: 3276\n","INFO 12-16 05:51:36 distributed_gpu_executor.py:61] Maximum concurrency for 8192 tokens per request: 2.91x\n","INFO 12-16 05:51:41 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n","INFO 12-16 05:51:41 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","\u001b[1;36m(VllmWorkerProcess pid=3496)\u001b[0;0m INFO 12-16 05:51:41 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n","\u001b[1;36m(VllmWorkerProcess pid=3496)\u001b[0;0m INFO 12-16 05:51:41 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","\u001b[1;36m(VllmWorkerProcess pid=3495)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3497)\u001b[0;0m INFO 12-16 05:51:41 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n","INFO 12-16 05:51:41 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n","\u001b[1;36m(VllmWorkerProcess pid=3495)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3497)\u001b[0;0m INFO 12-16 05:51:41 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","INFO 12-16 05:51:41 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","INFO 12-16 05:52:08 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n","\u001b[1;36m(VllmWorkerProcess pid=3497)\u001b[0;0m INFO 12-16 05:52:08 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n","\u001b[1;36m(VllmWorkerProcess pid=3495)\u001b[0;0m INFO 12-16 05:52:08 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n","\u001b[1;36m(VllmWorkerProcess pid=3496)\u001b[0;0m INFO 12-16 05:52:09 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n","\u001b[1;36m(VllmWorkerProcess pid=3497)\u001b[0;0m INFO 12-16 05:52:09 model_runner.py:1518] Graph capturing finished in 28 secs, took 0.42 GiB\n","INFO 12-16 05:52:09 model_runner.py:1518] Graph capturing finished in 28 secs, took 0.42 GiB\n","\u001b[1;36m(VllmWorkerProcess pid=3495)\u001b[0;0m INFO 12-16 05:52:09 model_runner.py:1518] Graph capturing finished in 28 secs, took 0.42 GiB\n","\u001b[1;36m(VllmWorkerProcess pid=3496)\u001b[0;0m INFO 12-16 05:52:09 model_runner.py:1518] Graph capturing finished in 28 secs, took 0.42 GiB\n"]}],"source":["from vllm import LLM, SamplingParams\n","\n","model_name_70b = 'PatronusAI/Llama-3-Patronus-Lynx-70B-Instruct'\n","\n","model_70b = LLM(model_name_70b, tensor_parallel_size=4, gpu_memory_utilization=0.98)"]},{"cell_type":"markdown","source":["### Halubench"],"metadata":{"id":"VeUxdZRjDgT3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHxtxshmLnTW","tags":[]},"outputs":[],"source":["from datasets import load_dataset, Dataset\n","\n","ds = load_dataset(\"PatronusAI/HaluBench\")\n","data = ds[\"test\"].to_pandas()\n","\n","# store sources for subsets\n","sources = data.source_ds.unique().tolist()\n","sources.remove('RAGTruth')\n","sources.remove('halueval')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tqqltJxSW4Uv","tags":[]},"outputs":[],"source":["# Lynx prompt template\n","def generate_prompt(question, document, answer):\n","    prompt = f\"\"\"\n","    Given the following QUESTION, DOCUMENT and ANSWER you must analyze the provided answer and determine whether it is faithful to the contents of the DOCUMENT. The ANSWER must not offer new information beyond the context provided in the DOCUMENT. The ANSWER also must not contradict information provided in the DOCUMENT. Output your final verdict by strictly following this format: \"PASS\" if the answer is faithful to the DOCUMENT and \"FAIL\" if the answer is not faithful to the DOCUMENT. Show your reasoning.\n","    --\n","    QUESTION (THIS DOES NOT COUNT AS BACKGROUND INFORMATION):\n","    {question}\n","    --\n","    DOCUMENT:\n","    {document}\n","    --\n","    ANSWER:\n","    {answer}\n","    --\n","    Your output should be in JSON FORMAT with the keys \"REASONING\" and \"SCORE\":\n","    {{\"REASONING\": \"<your reasoning as bullet points>\", \"SCORE\": \"<your final score>\"}}\n","    \"\"\"\n","    return prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBGO_VryW6s2"},"outputs":[],"source":["import os, json\n","\n","def save_to_json(filename, reasonings, scores, source):\n","    # check if json exists\n","    if os.path.exists(f'{filename}.json'):\n","        with open(f'{filename}.json', 'r') as f:\n","            results_dict = json.load(f)\n","    else:\n","        results_dict = {}\n","\n","    # add the results under source key\n","    results_dict[source] = [{'reasoning': reasoning, 'score': score} for reasoning, score in zip(reasonings, scores)]\n","\n","    # write the updated dictionary\n","    with open(f'{filename}.json', 'w') as f:\n","        json.dump(results_dict, f, indent=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0x5bqQxLaQok"},"outputs":[],"source":["from tqdm import tqdm\n","import re\n","\n","# extract reasoning and score from LLM response\n","def postprocess(results):\n","  reasonings = []\n","  scores = []\n","  none_score = 0\n","\n","  reasoning_pattern = r'\"REASONING\":\\s*\\[(.*?)\\]'\n","  score_pattern = r'\"SCORE\":\\s*(\\w+)'\n","\n","  for response in results:\n","      text = response.outputs[0].text\n","      reasoning_match = re.search(reasoning_pattern, text, re.DOTALL)\n","      reasoning = [r.strip(\"'\") for r in reasoning_match.group(1).split(\"', '\")] if reasoning_match else None\n","      reasonings.append(reasoning)\n","\n","      score_match = re.search(score_pattern, text)\n","      if score_match:\n","          score = score_match.group(1)\n","          if score == \"PASS\":\n","              score_num = 1\n","          elif score == \"FAIL\":\n","              score_num = 0\n","          else:\n","              score_num = None\n","      else:\n","          none_score+=1\n","          score_num = None\n","      scores.append(score_num)\n","\n","  print(\"Responses without score: \", none_score)\n","  return reasonings, scores\n","\n","def parse_output(response):\n","    reasoning_pattern = r'\"REASONING\":\\s*\\[(.*?)\\]'\n","    score_pattern = r'\"SCORE\":\\s*(\\w+)'\n","\n","    text = response.outputs[0].text\n","    reasoning_match = re.search(reasoning_pattern, text, re.DOTALL)\n","    reasoning = [r.strip(\"'\") for r in reasoning_match.group(1).split(\"', '\")] if reasoning_match else None\n","\n","    score_match = re.search(score_pattern, text)\n","    if score_match:\n","        score = score_match.group(1)\n","        if score == \"PASS\":\n","            score_num = 1\n","        elif score == \"FAIL\":\n","            score_num = 0\n","        else:\n","            score_num = None\n","    else:\n","        score_num = None\n","\n","    return reasoning, score_num"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6B4fN6dCNkA"},"outputs":[],"source":["def run_metrics(data, sources, model, filename):\n","    results = {}\n","    max_retries = 10\n","    for source in sources:\n","        # extract subset\n","        subset = data[data.source_ds == source]\n","\n","        prompts = [generate_prompt(question, document, answer) for question, document, answer in zip(subset['question'], subset['passage'], subset['answer'])]\n","        params = SamplingParams(max_tokens=1024)\n","\n","        responses = model.generate(prompts, params, use_tqdm=True)\n","\n","        to_retry_inputs = []\n","        to_retry_indices = []\n","        for i, resp in enumerate(responses):\n","            feedback, score = parse_output(resp)\n","            if feedback is None:\n","                to_retry_inputs.append(prompts[i])\n","                to_retry_indices.append(i)\n","\n","        # Retry logic with progress bar\n","        retries = 0\n","        while to_retry_inputs and retries < max_retries:\n","            retries += 1\n","            print(f\"Retrying failed batches: Attempt {retries}/{max_retries}\")\n","            retry_outputs = model.generate(to_retry_inputs, params, use_tqdm=True)\n","\n","            new_to_retry_inputs = []\n","            new_to_retry_indices = []\n","            for idx, (retry_idx, output) in enumerate(zip(to_retry_indices, retry_outputs)):\n","                feedback, score = parse_output(output)\n","                if feedback is None:  # Still failing\n","                    new_to_retry_inputs.append(to_retry_inputs[idx])\n","                    new_to_retry_indices.append(to_retry_indices[idx])\n","                else:\n","                    responses[retry_idx] = output  # Update with successful retry\n","\n","            to_retry_inputs = new_to_retry_inputs\n","            to_retry_indices = new_to_retry_indices\n","\n","        results[source] = responses\n","        # postprocess to again extract reasoning and score\n","        reasonings, scores = postprocess(responses)\n","        # save results to json\n","        save_to_json(filename, reasonings, scores, source)\n","\n","        print(\"Completed evaluation on {0} dataset. Length of feedback: {1} and scores: {2}\".format(source, len(reasonings), len(scores)))\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"3e3bQQ8ICNkA","outputId":"9cfbd5f5-9f00-4f08-bf81-d028b8a2f24c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Processed prompts:   0%|                                        | 2/1000 [00:03<27:40,  1.66s/it, est. speed input: 228.07 toks/s, output: 0.50 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:18:29 scheduler.py:1481] Sequence group 86 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   9%|███▎                                 | 89/1000 [01:15<14:54,  1.02it/s, est. speed input: 554.20 toks/s, output: 405.79 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:19:41 scheduler.py:1481] Sequence group 150 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  16%|█████▋                              | 159/1000 [02:15<13:55,  1.01it/s, est. speed input: 537.97 toks/s, output: 453.09 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:20:41 scheduler.py:1481] Sequence group 218 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  25%|█████████▏                          | 254/1000 [03:43<06:25,  1.93it/s, est. speed input: 524.39 toks/s, output: 494.77 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:22:08 scheduler.py:1481] Sequence group 320 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  33%|███████████▊                        | 327/1000 [04:52<10:40,  1.05it/s, est. speed input: 518.60 toks/s, output: 498.20 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:23:18 scheduler.py:1481] Sequence group 390 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  40%|██████████████▎                     | 397/1000 [05:59<10:39,  1.06s/it, est. speed input: 509.70 toks/s, output: 501.95 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:24:28 scheduler.py:1481] Sequence group 452 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  50%|██████████████████                  | 501/1000 [07:23<06:30,  1.28it/s, est. speed input: 520.44 toks/s, output: 512.28 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:25:48 scheduler.py:1481] Sequence group 563 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  57%|████████████████████▋               | 573/1000 [08:30<02:57,  2.40it/s, est. speed input: 514.39 toks/s, output: 519.72 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:26:56 scheduler.py:1481] Sequence group 639 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  65%|███████████████████████▎            | 648/1000 [09:42<05:17,  1.11it/s, est. speed input: 510.82 toks/s, output: 517.61 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:28:08 scheduler.py:1481] Sequence group 710 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  71%|█████████████████████████▋          | 714/1000 [10:46<03:58,  1.20it/s, est. speed input: 504.75 toks/s, output: 521.84 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:29:13 scheduler.py:1481] Sequence group 774 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  78%|████████████████████████████        | 778/1000 [12:01<04:32,  1.23s/it, est. speed input: 494.10 toks/s, output: 518.95 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:30:27 scheduler.py:1481] Sequence group 837 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  89%|███████████████████████████████▉    | 888/1000 [13:29<00:44,  2.54it/s, est. speed input: 506.00 toks/s, output: 523.17 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:31:54 scheduler.py:1481] Sequence group 952 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  97%|███████████████████████████████████ | 973/1000 [14:38<00:33,  1.22s/it, est. speed input: 510.42 toks/s, output: 519.81 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:33:04 scheduler.py:1481] Sequence group 1031 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████| 1000/1000 [15:24<00:00,  1.08it/s, est. speed input: 498.63 toks/s, output: 517.75 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  17%|██████▍                               | 64/379 [00:59<05:25,  1.03s/it, est. speed input: 513.28 toks/s, output: 387.95 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:34:49 scheduler.py:1481] Sequence group 1134 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  32%|███████████▉                         | 122/379 [02:14<12:11,  2.85s/it, est. speed input: 426.34 toks/s, output: 420.29 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:36:04 scheduler.py:1481] Sequence group 1173 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  53%|███████████████████▌                 | 201/379 [03:36<01:22,  2.15it/s, est. speed input: 436.57 toks/s, output: 480.95 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:37:26 scheduler.py:1481] Sequence group 1266 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  68%|█████████████████████████            | 257/379 [04:34<02:29,  1.23s/it, est. speed input: 437.84 toks/s, output: 484.73 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:38:31 scheduler.py:1481] Sequence group 1309 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|█████████████████████████████████████| 379/379 [06:48<00:00,  1.08s/it, est. speed input: 435.76 toks/s, output: 505.78 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 2/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   1%|▎                                        | 1/141 [00:04<09:36,  4.12s/it, est. speed input: 107.83 toks/s, output: 0.73 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:40:43 scheduler.py:1481] Sequence group 1460 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  62%|███████████████████████▍              | 87/141 [01:21<01:12,  1.34s/it, est. speed input: 513.27 toks/s, output: 389.89 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:42:00 scheduler.py:1481] Sequence group 1524 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|█████████████████████████████████████| 141/141 [02:24<00:00,  1.03s/it, est. speed input: 461.48 toks/s, output: 478.33 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 3/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████| 59/59 [01:27<00:00,  1.48s/it, est. speed input: 323.56 toks/s, output: 362.83 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 4/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████| 23/23 [00:40<00:00,  1.77s/it, est. speed input: 286.65 toks/s, output: 246.70 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 5/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████| 10/10 [00:38<00:00,  3.80s/it, est. speed input: 146.73 toks/s, output: 124.27 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 6/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████████| 2/2 [00:19<00:00,  9.53s/it, est. speed input: 84.02 toks/s, output: 41.62 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 7/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████████| 1/1 [00:06<00:00,  6.78s/it, est. speed input: 87.06 toks/s, output: 28.18 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 8/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████████████████████████████████████| 1/1 [00:00<00:00,  5.97it/s, est. speed input: 3536.11 toks/s, output: 5.99 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 9/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████████| 1/1 [00:35<00:00, 35.77s/it, est. speed input: 16.49 toks/s, output: 28.63 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 10/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████████████████████████████████████| 1/1 [00:05<00:00,  5.70s/it, est. speed input: 103.56 toks/s, output: 28.08 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Responses without score:  2\n","Completed evaluation on DROP dataset. Length of feedback: 1000 and scores: 1000\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   0%|                                        | 2/1000 [00:04<33:47,  2.03s/it, est. speed input: 234.79 toks/s, output: 0.71 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:47:08 scheduler.py:1481] Sequence group 1689 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   9%|███▍                                 | 93/1000 [01:44<08:55,  1.69it/s, est. speed input: 474.18 toks/s, output: 406.99 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:48:43 scheduler.py:1481] Sequence group 1773 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  19%|██████▉                             | 193/1000 [03:12<11:42,  1.15it/s, est. speed input: 548.62 toks/s, output: 443.63 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:50:12 scheduler.py:1481] Sequence group 1870 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1051\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  29%|██████████▎                         | 288/1000 [04:51<07:19,  1.62it/s, est. speed input: 546.27 toks/s, output: 458.41 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:51:51 scheduler.py:1481] Sequence group 1965 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1101\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  36%|████████████▉                       | 361/1000 [06:18<15:46,  1.48s/it, est. speed input: 529.66 toks/s, output: 453.89 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:53:19 scheduler.py:1481] Sequence group 2040 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1151\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  45%|████████████████▎                   | 454/1000 [07:51<19:41,  2.16s/it, est. speed input: 537.58 toks/s, output: 458.88 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:54:53 scheduler.py:1481] Sequence group 2126 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1201\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  53%|███████████████████▏                | 534/1000 [09:23<07:16,  1.07it/s, est. speed input: 530.50 toks/s, output: 463.44 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:56:22 scheduler.py:1481] Sequence group 2215 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1251\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  61%|█████████████████████▉              | 609/1000 [10:57<02:39,  2.45it/s, est. speed input: 516.91 toks/s, output: 467.56 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:57:55 scheduler.py:1481] Sequence group 2293 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1301\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  71%|█████████████████████████▌          | 710/1000 [12:41<01:36,  3.02it/s, est. speed input: 519.96 toks/s, output: 469.18 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 07:59:40 scheduler.py:1481] Sequence group 2395 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1351\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  78%|████████████████████████████        | 781/1000 [14:02<05:34,  1.53s/it, est. speed input: 516.27 toks/s, output: 463.47 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:01:02 scheduler.py:1481] Sequence group 2454 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1401\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  86%|███████████████████████████████▏    | 865/1000 [15:31<02:58,  1.32s/it, est. speed input: 516.42 toks/s, output: 467.48 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:02:32 scheduler.py:1481] Sequence group 2537 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1451\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  96%|██████████████████████████████████▌ | 961/1000 [17:06<00:13,  2.80it/s, est. speed input: 521.10 toks/s, output: 472.08 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:04:05 scheduler.py:1481] Sequence group 2645 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1501\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████| 1000/1000 [18:08<00:00,  1.09s/it, est. speed input: 512.22 toks/s, output: 468.86 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  14%|█████▎                                | 61/433 [00:55<04:48,  1.29it/s, est. speed input: 608.32 toks/s, output: 375.94 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:06:04 scheduler.py:1481] Sequence group 2746 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1551\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  31%|███████████▌                         | 135/433 [02:11<04:04,  1.22it/s, est. speed input: 572.11 toks/s, output: 420.30 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:07:22 scheduler.py:1481] Sequence group 2809 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1601\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  53%|███████████████████▍                 | 228/433 [03:50<02:57,  1.15it/s, est. speed input: 556.62 toks/s, output: 442.37 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:08:59 scheduler.py:1481] Sequence group 2907 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1651\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  71%|██████████████████████████▏          | 307/433 [05:22<01:13,  1.71it/s, est. speed input: 536.77 toks/s, output: 456.21 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:10:31 scheduler.py:1481] Sequence group 2990 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1701\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  94%|██████████████████████████████████▌  | 405/433 [06:53<00:27,  1.04it/s, est. speed input: 550.03 toks/s, output: 455.10 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:12:03 scheduler.py:1481] Sequence group 3079 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1751\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|█████████████████████████████████████| 433/433 [07:40<00:00,  1.06s/it, est. speed input: 529.71 toks/s, output: 457.74 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 2/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  37%|██████████████▏                       | 67/180 [01:25<02:31,  1.34s/it, est. speed input: 434.74 toks/s, output: 356.44 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:14:16 scheduler.py:1481] Sequence group 3172 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1801\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  84%|███████████████████████████████▏     | 152/180 [02:59<00:33,  1.20s/it, est. speed input: 481.19 toks/s, output: 423.36 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:15:51 scheduler.py:1481] Sequence group 3259 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1851\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|█████████████████████████████████████| 180/180 [03:58<00:00,  1.33s/it, est. speed input: 429.93 toks/s, output: 417.78 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 3/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████| 85/85 [01:40<00:00,  1.19s/it, est. speed input: 490.82 toks/s, output: 388.08 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 4/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████| 40/40 [00:53<00:00,  1.35s/it, est. speed input: 441.84 toks/s, output: 404.86 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 5/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████| 17/17 [00:41<00:00,  2.42s/it, est. speed input: 253.34 toks/s, output: 264.78 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 6/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|█████████████████████████████████████████| 8/8 [00:37<00:00,  4.66s/it, est. speed input: 129.52 toks/s, output: 125.31 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 7/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████████████████████████████████████| 3/3 [00:10<00:00,  3.63s/it, est. speed input: 178.54 toks/s, output: 67.48 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 8/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████████| 2/2 [00:22<00:00, 11.48s/it, est. speed input: 55.47 toks/s, output: 36.16 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Responses without score:  4\n","Completed evaluation on pubmedQA dataset. Length of feedback: 1000 and scores: 1000\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   3%|█                                    | 30/1000 [00:40<32:04,  1.98s/it, est. speed input: 689.16 toks/s, output: 185.96 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:22:03 scheduler.py:1481] Sequence group 3463 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1901\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  20%|███████▎                            | 203/1000 [04:31<25:07,  1.89s/it, est. speed input: 716.21 toks/s, output: 308.88 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:25:53 scheduler.py:1481] Sequence group 3639 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1951\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  35%|████████████▋                       | 352/1000 [08:05<08:20,  1.29it/s, est. speed input: 689.36 toks/s, output: 331.85 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:29:24 scheduler.py:1481] Sequence group 3793 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2001\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  51%|██████████████████▏                 | 506/1000 [11:39<12:51,  1.56s/it, est. speed input: 684.63 toks/s, output: 332.86 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:32:58 scheduler.py:1481] Sequence group 3944 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2051\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  65%|███████████████████████▎            | 647/1000 [14:58<11:21,  1.93s/it, est. speed input: 680.69 toks/s, output: 339.41 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:36:16 scheduler.py:1481] Sequence group 4086 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2101\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  79%|████████████████████████████▌       | 793/1000 [18:24<04:48,  1.39s/it, est. speed input: 671.59 toks/s, output: 340.40 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:39:43 scheduler.py:1481] Sequence group 4231 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2151\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  95%|██████████████████████████████████▎ | 952/1000 [21:53<01:03,  1.32s/it, est. speed input: 676.65 toks/s, output: 342.94 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:43:12 scheduler.py:1481] Sequence group 4390 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2201\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████| 1000/1000 [23:17<00:00,  1.40s/it, est. speed input: 670.66 toks/s, output: 342.37 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  19%|███████▏                              | 95/499 [02:42<06:14,  1.08it/s, est. speed input: 576.54 toks/s, output: 313.77 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:47:19 scheduler.py:1481] Sequence group 4533 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2251\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  44%|████████████████▍                    | 221/499 [05:51<04:33,  1.02it/s, est. speed input: 620.52 toks/s, output: 316.11 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:50:30 scheduler.py:1481] Sequence group 4660 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2301\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  78%|████████████████████████████▊        | 389/499 [10:07<04:33,  2.49s/it, est. speed input: 629.43 toks/s, output: 321.15 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:54:44 scheduler.py:1481] Sequence group 4824 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2351\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|█████████████████████████████████████| 499/499 [13:10<00:00,  1.58s/it, est. speed input: 628.33 toks/s, output: 323.14 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 2/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  20%|███████▍                              | 50/254 [01:13<04:00,  1.18s/it, est. speed input: 670.46 toks/s, output: 260.17 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 08:59:01 scheduler.py:1481] Sequence group 4986 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2401\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  85%|███████████████████████████████▌     | 217/254 [05:37<00:53,  1.46s/it, est. speed input: 677.10 toks/s, output: 300.45 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 09:03:25 scheduler.py:1481] Sequence group 5149 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2451\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|█████████████████████████████████████| 254/254 [07:02<00:00,  1.66s/it, est. speed input: 652.46 toks/s, output: 292.92 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 3/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  84%|███████████████████████████████      | 109/130 [03:05<00:34,  1.64s/it, est. speed input: 649.18 toks/s, output: 289.69 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 09:07:59 scheduler.py:1481] Sequence group 5297 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2501\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|█████████████████████████████████████| 130/130 [04:06<00:00,  1.90s/it, est. speed input: 595.40 toks/s, output: 285.53 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 4/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████| 72/72 [02:19<00:00,  1.94s/it, est. speed input: 637.38 toks/s, output: 245.44 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 5/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████| 38/38 [01:31<00:00,  2.41s/it, est. speed input: 543.24 toks/s, output: 238.86 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 6/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████| 21/21 [00:44<00:00,  2.10s/it, est. speed input: 662.23 toks/s, output: 189.97 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 7/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████| 11/11 [00:40<00:00,  3.64s/it, est. speed input: 399.57 toks/s, output: 124.21 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 8/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████████████████████████████████████| 6/6 [00:37<00:00,  6.25s/it, est. speed input: 211.83 toks/s, output: 64.48 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 9/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████████████████████████████████████| 3/3 [00:20<00:00,  6.80s/it, est. speed input: 211.27 toks/s, output: 37.43 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 10/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████████████████████████████████████| 3/3 [00:36<00:00, 12.24s/it, est. speed input: 117.47 toks/s, output: 63.61 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Responses without score:  5\n","Completed evaluation on FinanceBench dataset. Length of feedback: 1000 and scores: 1000\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  23%|████████▍                           | 233/1000 [17:18<53:11,  4.16s/it, est. speed input: 903.28 toks/s, output: 105.12 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 09:33:18 scheduler.py:1481] Sequence group 5695 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2551\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  81%|███████████████████████████▌      | 809/1000 [1:02:31<17:00,  5.34s/it, est. speed input: 870.39 toks/s, output: 105.34 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 10:18:46 scheduler.py:1481] Sequence group 6272 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2601\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|█████████████████████████████████| 1000/1000 [1:17:36<00:00,  4.66s/it, est. speed input: 864.77 toks/s, output: 105.60 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  67%|████████████████████████▉            | 442/657 [35:45<22:21,  6.24s/it, est. speed input: 839.57 toks/s, output: 104.83 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-09 11:09:28 scheduler.py:1481] Sequence group 6904 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2651\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|█████████████████████████████████████| 657/657 [54:50<00:00,  5.01s/it, est. speed input: 817.46 toks/s, output: 104.42 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 2/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:  36%|█████████████▎                       | 161/448 [13:53<18:05,  3.78s/it, est. speed input: 808.82 toks/s, output: 100.56 toks/s]"]}],"source":["filename = 'lynx_70b'\n","\n","results = run_metrics(data, sources, model_70b, filename)"]},{"cell_type":"markdown","source":["### ELI5"],"metadata":{"id":"5iqwtX-MDoQK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2f73FnRCNkA"},"outputs":[],"source":["def run_metrics(data, model, filename):\n","    results = {}\n","    max_retries = 10\n","    source = 'eli5'\n","    prompts = [generate_prompt(question, document, answer) for question, document, answer in zip(data['user_input'], data['reference'], data['response'])]\n","    params = SamplingParams(max_tokens=1024)\n","\n","    responses = model.generate(prompts, params, use_tqdm=True)\n","\n","    to_retry_inputs = []\n","    to_retry_indices = []\n","    for i, resp in enumerate(responses):\n","        feedback, score = parse_output(resp)\n","        if feedback is None:\n","            to_retry_inputs.append(prompts[i])\n","            to_retry_indices.append(i)\n","\n","    # Retry logic with progress bar\n","    retries = 0\n","    while to_retry_inputs and retries < max_retries:\n","        retries += 1\n","        print(f\"Retrying failed batches: Attempt {retries}/{max_retries}\")\n","        retry_outputs = model.generate(to_retry_inputs, params, use_tqdm=True)\n","\n","        new_to_retry_inputs = []\n","        new_to_retry_indices = []\n","        for idx, (retry_idx, output) in enumerate(zip(to_retry_indices, retry_outputs)):\n","            feedback, score = parse_output(output)\n","            if feedback is None:  # Still failing\n","                new_to_retry_inputs.append(to_retry_inputs[idx])\n","                new_to_retry_indices.append(to_retry_indices[idx])\n","            else:\n","                responses[retry_idx] = output  # Update with successful retry\n","\n","        to_retry_inputs = new_to_retry_inputs\n","        to_retry_indices = new_to_retry_indices\n","\n","    results[source] = responses\n","    # postprocess to extract reasoning and score\n","    reasonings, scores = postprocess(responses)\n","    # save results to json\n","    save_to_json(filename, reasonings, scores, source)\n","\n","    print(\"Completed evaluation on {0} dataset. Length of feedback: {1} and scores: {2}\".format(source, len(reasonings), len(scores)))\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZJd1M70CNkB","outputId":"ad115592-1291-4601-dffe-2747665d44f6"},"outputs":[{"name":"stderr","output_type":"stream","text":["Processed prompts:   3%|██                                                                    | 3/100 [00:05<02:18,  1.43s/it, est. speed input: 219.47 toks/s, output: 0.56 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-16 05:53:30 scheduler.py:1481] Sequence group 58 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████████████████████████████████████████████████████████████| 100/100 [01:38<00:00,  1.02it/s, est. speed input: 413.89 toks/s, output: 544.98 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|████████████████████████████████████████████████████████████████████| 38/38 [00:43<00:00,  1.16s/it, est. speed input: 351.71 toks/s, output: 494.67 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 2/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|████████████████████████████████████████████████████████████████████| 14/14 [00:39<00:00,  2.79s/it, est. speed input: 146.95 toks/s, output: 211.57 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 3/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:36<00:00,  6.16s/it, est. speed input: 69.29 toks/s, output: 123.36 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 4/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|████████████████████████████████████████████████████████████████████████| 3/3 [00:27<00:00,  9.31s/it, est. speed input: 45.13 toks/s, output: 39.72 toks/s]"]},{"name":"stdout","output_type":"stream","text":["Responses without score:  0\n","Completed evaluation on eli5 dataset. Length of feedback: 100 and scores: 100\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from datasets import load_dataset\n","\n","ds = load_dataset(\"explodinggradients/ELI5\")\n","data = ds[\"train\"].to_pandas()\n","\n","filename = 'lynx_70b'\n","\n","results = run_metrics(data, model_70b, filename)"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat":4,"nbformat_minor":0}